{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffb1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15be2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e989eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e0f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c614f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am a MachineLearning Engineer and work in Barcelona'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f751e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e5c0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'work', 'in', 'barcelona']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb428f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ef6b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'am', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'work', 'in', 'barcelona', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788e6ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'am', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'work', 'in', 'barcelona', '[SEP]', '[PAD]', '[PAD]']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d1d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6df316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2572, 1037, 3698, 19738, 6826, 2075, 3992, 1998, 2147, 1999, 7623, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# These tokens represent each word\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d26aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each token has a vector, all tokens vector is the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48113047",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3db10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids, attention_mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32caf260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.5441e-02,  5.0635e-01, -1.1320e-03,  ..., -3.2490e-01,\n",
       "           5.9749e-01,  3.5463e-01],\n",
       "         [ 3.0242e-01,  2.2726e-01, -2.7193e-01,  ...,  4.2097e-04,\n",
       "           5.2820e-01,  1.5037e-01],\n",
       "         [ 2.6702e-01,  4.4990e-01,  4.7700e-01,  ..., -5.9956e-01,\n",
       "           3.5239e-01,  5.3516e-01],\n",
       "         ...,\n",
       "         [ 6.0083e-01, -4.7625e-02, -5.3985e-01,  ...,  1.4627e-01,\n",
       "          -6.9843e-01, -5.1320e-01],\n",
       "         [-1.0425e-01,  2.8543e-01,  2.1125e-01,  ..., -1.5509e-01,\n",
       "           7.4580e-01, -1.3839e-01],\n",
       "         [-2.2645e-01,  1.3089e-01,  1.7929e-01,  ...,  1.7237e-01,\n",
       "           6.9892e-01, -1.7191e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8937, -0.6179, -0.9818,  0.8609,  0.8978, -0.2795,  0.8637,  0.4791,\n",
       "         -0.8812, -1.0000, -0.6820,  0.9861,  0.9729,  0.8106,  0.8969, -0.8738,\n",
       "         -0.3556, -0.7230,  0.4542, -0.3178,  0.7620,  1.0000, -0.1531,  0.4714,\n",
       "          0.5821,  0.9975, -0.8294,  0.8970,  0.9494,  0.7455, -0.6994,  0.3336,\n",
       "         -0.9916, -0.2466, -0.9857, -0.9953,  0.6701, -0.7367, -0.1404, -0.1214,\n",
       "         -0.8733,  0.5274,  1.0000, -0.0490,  0.7080, -0.2878, -1.0000,  0.3411,\n",
       "         -0.8943,  0.9757,  0.9580,  0.9497,  0.3875,  0.6210,  0.6355, -0.5283,\n",
       "          0.0270,  0.2576, -0.4402, -0.7053, -0.7129,  0.6000, -0.9566, -0.8952,\n",
       "          0.9598,  0.9412, -0.3107, -0.4197, -0.1723, -0.0673,  0.9247,  0.3284,\n",
       "         -0.3785, -0.8584,  0.8204,  0.3270, -0.7937,  1.0000, -0.6636, -0.9759,\n",
       "          0.9713,  0.9092,  0.7422, -0.5814,  0.7921, -1.0000,  0.7542, -0.2363,\n",
       "         -0.9920,  0.3261,  0.7283, -0.3517,  0.9104,  0.7958, -0.7494, -0.7298,\n",
       "         -0.5203, -0.9529, -0.4291, -0.5743,  0.2791, -0.3511, -0.5490, -0.4899,\n",
       "          0.3672, -0.7220, -0.5008,  0.6927,  0.4085,  0.6886,  0.6488, -0.5604,\n",
       "          0.4264, -0.9419,  0.6949, -0.5307, -0.9863, -0.7916, -0.9873,  0.8101,\n",
       "         -0.6661, -0.3879,  0.9580, -0.4105,  0.6222, -0.1993, -0.9796, -1.0000,\n",
       "         -0.8133, -0.7806, -0.3234, -0.4446, -0.9769, -0.9695,  0.6916,  0.9410,\n",
       "          0.4764,  1.0000, -0.5557,  0.9390, -0.4332, -0.8423,  0.7763, -0.6325,\n",
       "          0.9211,  0.4214, -0.6414,  0.3631, -0.7781,  0.4447, -0.8857, -0.2078,\n",
       "         -0.9132, -0.9225, -0.4732,  0.9394, -0.8778, -0.9877, -0.2967, -0.4180,\n",
       "         -0.6318,  0.8201,  0.8896,  0.5289, -0.5264,  0.5982,  0.6000,  0.6890,\n",
       "         -0.8239, -0.3451,  0.6236, -0.4842, -0.9756, -0.9742, -0.4749,  0.5263,\n",
       "          0.9860,  0.7346,  0.5090,  0.8992, -0.5274,  0.8400, -0.9562,  0.9832,\n",
       "         -0.3315,  0.3550, -0.8605,  0.5159, -0.8851,  0.4541,  0.8453, -0.8808,\n",
       "         -0.8053, -0.2666, -0.6296, -0.4529, -0.9502,  0.5174, -0.4046, -0.5528,\n",
       "         -0.3370,  0.9296,  0.9764,  0.7357,  0.5176,  0.6813, -0.9135, -0.6157,\n",
       "          0.1875,  0.3700,  0.3324,  0.9873, -0.9232, -0.2989, -0.9089, -0.9857,\n",
       "         -0.0240, -0.8769, -0.2996, -0.8439,  0.8104, -0.7822,  0.7138,  0.5195,\n",
       "         -0.9749, -0.7967,  0.4671, -0.6702,  0.5937, -0.5120,  0.9620,  0.9872,\n",
       "         -0.7031,  0.3471,  0.9627, -0.9903, -0.8479,  0.7982, -0.4670,  0.8958,\n",
       "         -0.7610,  0.9965,  0.9848,  0.8942, -0.8877, -0.9307, -0.8897, -0.9011,\n",
       "         -0.1440,  0.5516,  0.9665,  0.8282,  0.5520, -0.0332, -0.6917,  0.9947,\n",
       "         -0.9755, -0.9586, -0.8247, -0.4415, -0.9902,  0.9533,  0.4683,  0.8682,\n",
       "         -0.5939, -0.8027, -0.9493,  0.8074,  0.3542,  0.9840, -0.6392, -0.9278,\n",
       "         -0.6863, -0.9380,  0.0218, -0.3863, -0.6961, -0.0093, -0.9472,  0.6683,\n",
       "          0.6840,  0.7573, -0.9501,  0.9981,  1.0000,  0.9764,  0.8793,  0.8585,\n",
       "         -1.0000, -0.7587,  1.0000, -0.9984, -1.0000, -0.9210, -0.8291,  0.5104,\n",
       "         -1.0000, -0.2520, -0.1255, -0.8980,  0.8720,  0.9559,  0.9857, -1.0000,\n",
       "          0.8213,  0.9202, -0.8023,  0.9894, -0.6279,  0.9596,  0.7269,  0.6529,\n",
       "         -0.4197,  0.5913, -0.9905, -0.8983, -0.8026, -0.9290,  0.9999,  0.2546,\n",
       "         -0.8872, -0.8635,  0.8402, -0.2248,  0.0484, -0.9668, -0.4082,  0.7716,\n",
       "          0.8816,  0.3572,  0.5893, -0.6283,  0.3922,  0.4398,  0.4788,  0.8176,\n",
       "         -0.9319, -0.5735, -0.2771, -0.1642, -0.8449, -0.9728,  0.9631, -0.5963,\n",
       "          0.9477,  1.0000,  0.5612, -0.9045,  0.8153,  0.4624, -0.8241,  1.0000,\n",
       "          0.9331, -0.9815, -0.7521,  0.8396, -0.7119, -0.8039,  0.9998, -0.3671,\n",
       "         -0.8433, -0.7643,  0.9865, -0.9877,  0.9998, -0.9149, -0.9655,  0.9612,\n",
       "          0.9328, -0.8379, -0.7936,  0.2562, -0.8850,  0.4267, -0.8928,  0.7569,\n",
       "          0.5748, -0.1897,  0.9128, -0.7598, -0.7817,  0.3738, -0.6959, -0.3159,\n",
       "          0.9919,  0.6023, -0.3675,  0.1197, -0.3418, -0.8913, -0.9629,  0.8195,\n",
       "          1.0000, -0.3997,  0.9625, -0.6804, -0.1419,  0.2054,  0.6785,  0.7057,\n",
       "         -0.4910, -0.9035,  0.8810, -0.9657, -0.9908,  0.6885,  0.3615, -0.4188,\n",
       "          1.0000,  0.7369,  0.3576,  0.4712,  0.9957,  0.1619,  0.5121,  0.9610,\n",
       "          0.9798, -0.3335,  0.7649,  0.7977, -0.9706, -0.5002, -0.7434,  0.0795,\n",
       "         -0.9421, -0.0963, -0.9632,  0.9637,  0.9874,  0.4375,  0.3963,  0.9304,\n",
       "          1.0000, -0.9319,  0.6644,  0.1858,  0.7214, -1.0000, -0.8724, -0.5816,\n",
       "         -0.2097, -0.9522, -0.5524,  0.2654, -0.9706,  0.9435,  0.8295, -0.9865,\n",
       "         -0.9822, -0.4057,  0.8949,  0.2526, -0.9982, -0.7478, -0.5609,  0.8775,\n",
       "         -0.4808, -0.9227, -0.3173, -0.4455,  0.5607, -0.3788,  0.7543,  0.9629,\n",
       "          0.4950, -0.9543, -0.6502, -0.2814, -0.8249,  0.8584, -0.8463, -0.9903,\n",
       "         -0.3815,  1.0000, -0.7002,  0.9716,  0.7561,  0.6917, -0.3919,  0.3957,\n",
       "          0.9911,  0.4679, -0.9100, -0.9573,  0.0282, -0.5434,  0.7639,  0.8768,\n",
       "          0.9121,  0.8360,  0.9171,  0.3966, -0.0883,  0.2375,  0.9989, -0.2486,\n",
       "         -0.3072, -0.5841, -0.4162, -0.5388, -0.0297,  1.0000,  0.3280,  0.7992,\n",
       "         -0.9902, -0.9719, -0.8929,  1.0000,  0.8399, -0.8257,  0.7925,  0.7893,\n",
       "         -0.3248,  0.8301, -0.3841, -0.3696,  0.4388,  0.1661,  0.9500, -0.7168,\n",
       "         -0.9718, -0.6765,  0.6456, -0.9700,  1.0000, -0.7137, -0.5289, -0.5827,\n",
       "         -0.5824, -0.3494,  0.0309, -0.9800, -0.3719,  0.2903,  0.9644,  0.4453,\n",
       "         -0.7508, -0.8524,  0.9411,  0.9103, -0.9767, -0.9267,  0.9538, -0.9700,\n",
       "          0.6619,  1.0000,  0.5577,  0.4520,  0.4323, -0.6135,  0.5266, -0.7305,\n",
       "          0.8174, -0.9554, -0.5331, -0.3846,  0.4080, -0.2083, -0.7232,  0.6354,\n",
       "          0.2569, -0.7122, -0.7456, -0.2574,  0.5606,  0.8461, -0.3674, -0.2815,\n",
       "          0.2450, -0.2356, -0.9437, -0.5612, -0.5909, -1.0000,  0.7387, -1.0000,\n",
       "          0.8013,  0.5405, -0.3362,  0.8118,  0.7609,  0.8687, -0.7143, -0.9190,\n",
       "          0.0742,  0.6678, -0.5646, -0.5466, -0.6694,  0.5352, -0.3125,  0.4364,\n",
       "         -0.7385,  0.8069, -0.4769,  1.0000,  0.3319, -0.7737, -0.9677,  0.4139,\n",
       "         -0.4341,  1.0000, -0.8342, -0.9414,  0.5743, -0.8775, -0.8805,  0.4793,\n",
       "          0.1243, -0.8868, -0.9913,  0.9356,  0.8145, -0.7365,  0.7173, -0.4008,\n",
       "         -0.7062,  0.1034,  0.9755,  0.9874,  0.6689,  0.9076, -0.3791, -0.6145,\n",
       "          0.9719,  0.3696,  0.3471,  0.2822,  1.0000,  0.4989, -0.9205, -0.2348,\n",
       "         -0.9788, -0.3675, -0.9198,  0.5029,  0.4371,  0.8997, -0.3547,  0.9461,\n",
       "         -0.9523,  0.0589, -0.7141, -0.8360,  0.4585, -0.9374, -0.9845, -0.9733,\n",
       "          0.7651, -0.5618, -0.2449,  0.3400,  0.0364,  0.6517,  0.5565, -1.0000,\n",
       "          0.9360,  0.5762,  0.9780,  0.9650,  0.8704,  0.6866,  0.4240, -0.9794,\n",
       "         -0.9794, -0.4954, -0.3660,  0.7425,  0.7748,  0.8997,  0.4828, -0.5271,\n",
       "         -0.7546, -0.8157, -0.9288, -0.9897,  0.6677, -0.7626, -0.9359,  0.9656,\n",
       "          0.2228, -0.2783, -0.4459, -0.8990,  0.8236,  0.8732,  0.3565,  0.1187,\n",
       "          0.5732,  0.8681,  0.9414,  0.9772, -0.9712,  0.8124, -0.9228,  0.5894,\n",
       "          0.9260, -0.9216,  0.3388,  0.7576, -0.5507,  0.3984, -0.3859, -0.9445,\n",
       "          0.7765, -0.4298,  0.7184, -0.6074, -0.1242, -0.5772, -0.2488, -0.7945,\n",
       "         -0.8095,  0.8080,  0.6438,  0.8777,  0.9633, -0.2445, -0.8320, -0.2870,\n",
       "         -0.9304, -0.9267,  0.8880, -0.1877, -0.6445,  0.9315,  0.0727,  0.9656,\n",
       "          0.5737, -0.4603, -0.4932, -0.8256,  0.8585, -0.8309, -0.7124, -0.7447,\n",
       "          0.8240,  0.4935,  1.0000, -0.9413, -0.9593, -0.6016, -0.5797,  0.6418,\n",
       "         -0.6903, -1.0000,  0.4445, -0.6972,  0.9291, -0.8631,  0.9723, -0.8102,\n",
       "         -0.9708, -0.4951,  0.8437,  0.9097, -0.6064, -0.7249,  0.7755, -0.1408,\n",
       "          0.9958,  0.7937, -0.6517, -0.1953,  0.8048, -0.9443, -0.7348,  0.9287]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d65aabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_hidden_state, hidden representation for each token\n",
    "# pooler_output, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7761a969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aed7849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd2857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27193d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2060f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2708, -0.2844,  0.4581],\n",
      "         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],\n",
      "         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],\n",
      "         ...,\n",
      "         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],\n",
      "         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],\n",
      "         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,\n",
      "         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,\n",
      "         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,\n",
      "          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,\n",
      "         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,\n",
      "         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,\n",
      "         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,\n",
      "         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,\n",
      "          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,\n",
      "         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,\n",
      "          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,\n",
      "         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,\n",
      "         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,\n",
      "          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,\n",
      "          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,\n",
      "         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,\n",
      "         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,\n",
      "          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,\n",
      "          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,\n",
      "         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,\n",
      "         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,\n",
      "         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,\n",
      "          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,\n",
      "         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,\n",
      "         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,\n",
      "         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,\n",
      "          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,\n",
      "         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,\n",
      "         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,\n",
      "         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,\n",
      "         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,\n",
      "         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,\n",
      "         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,\n",
      "         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,\n",
      "         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,\n",
      "          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,\n",
      "         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,\n",
      "         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,\n",
      "          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,\n",
      "         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,\n",
      "         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,\n",
      "          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,\n",
      "         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,\n",
      "          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,\n",
      "          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,\n",
      "         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,\n",
      "          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,\n",
      "          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,\n",
      "          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,\n",
      "          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,\n",
      "         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,\n",
      "          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,\n",
      "          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,\n",
      "         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,\n",
      "          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,\n",
      "         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,\n",
      "         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,\n",
      "         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,\n",
      "          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,\n",
      "         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,\n",
      "          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,\n",
      "          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,\n",
      "         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,\n",
      "         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,\n",
      "         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,\n",
      "         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,\n",
      "         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,\n",
      "         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,\n",
      "          0.7551,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,\n",
      "          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,\n",
      "          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,\n",
      "          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,\n",
      "          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,\n",
      "          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,\n",
      "         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,\n",
      "         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9180, -0.8493,  0.5868,\n",
      "          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,\n",
      "         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1227, -0.5258,\n",
      "          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,\n",
      "         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,\n",
      "         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,\n",
      "          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,\n",
      "          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,\n",
      "         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,\n",
      "         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,\n",
      "         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,\n",
      "          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,\n",
      "          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,\n",
      "          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,\n",
      "         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,\n",
      "         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,\n",
      "          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,\n",
      "          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,\n",
      "         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,\n",
      "         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,\n",
      "          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n",
    "print(output)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b108ff1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (BertModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26724\\3744798790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\fjdur\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fjdur\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m         \u001b[1;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1210\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m         \u001b[1;31m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fjdur\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgenerate_compatible_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m                 \u001b[0mexception_message\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_model_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (BertModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}"
     ]
    }
   ],
   "source": [
    "out = model.generate(**encoded_input,max_length = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8da8636",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'prediction_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26724\\94439714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mseq_relationship_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_relationship_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_relationship_logits\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'prediction_logits'"
     ]
    }
   ],
   "source": [
    "prediction_logits = output.prediction_logits\n",
    "print(prediction_logits)\n",
    "seq_relationship_logits = output.seq_relationship_logits\n",
    "print(seq_relationship_logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1768f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26724\\663779126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "logits = output.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7888068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de213d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
      "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
      "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
      "         ...,\n",
      "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
      "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
      "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
      "          2.2183e-02,  5.7299e-01,  6.2331e-02,  5.9585e-02, -9.9965e-01,\n",
      "          5.0145e-02,  4.4756e-01,  9.7612e-01,  3.3989e-02,  8.4494e-01,\n",
      "         -3.6905e-01,  9.8648e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
      "          4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
      "          6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
      "         -3.2187e-01,  8.6673e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
      "         -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
      "         -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1518e-02,\n",
      "         -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3121e-02,\n",
      "          3.6740e-01, -7.3228e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
      "          4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
      "         -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9814e-01,\n",
      "          1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
      "          6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
      "          8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
      "          3.9848e-01,  2.1361e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
      "         -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
      "          3.9138e-01, -2.1885e-02, -9.6330e-02,  3.8545e-01, -3.4136e-01,\n",
      "         -8.0363e-02, -3.2023e-02, -3.6328e-01, -7.8130e-02,  1.9191e-02,\n",
      "         -1.3429e-01, -1.6014e-02, -5.2640e-02, -2.8006e-01,  9.3612e-02,\n",
      "         -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
      "          2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
      "         -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
      "          3.7343e-02, -1.9320e-01,  9.1691e-01,  3.6011e-01,  1.4505e-01,\n",
      "          1.5398e-01, -1.0659e-02, -1.0000e+00, -3.1573e-01, -3.1038e-01,\n",
      "          1.6523e-01, -8.0331e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
      "          9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
      "          2.5317e-01, -2.0185e-01,  2.9534e-02, -2.3162e-01,  3.4632e-01,\n",
      "         -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1135e-02,\n",
      "         -9.6284e-02, -7.6358e-02, -6.5151e-02, -8.9277e-01, -2.3465e-01,\n",
      "          9.1176e-01,  7.0428e-02, -2.1430e-01,  3.8197e-01,  3.5892e-02,\n",
      "         -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
      "          2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
      "          1.8090e-01, -3.2497e-02, -3.0138e-01, -9.7103e-01, -1.3917e-01,\n",
      "          3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8812e-02,  1.3992e-02,\n",
      "         -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
      "          1.5224e-01, -1.8241e-01,  5.5585e-02, -7.7306e-01, -9.8999e-02,\n",
      "          4.7058e-01, -1.7023e-01, -7.7803e-01,  5.2833e-02, -3.7679e-01,\n",
      "         -4.1296e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
      "          5.0383e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
      "          2.8564e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
      "          3.1903e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
      "         -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0621e-03, -4.1152e-01,\n",
      "          3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
      "         -4.8133e-01,  9.3246e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
      "          7.7973e-01,  4.6697e-01, -3.4636e-01,  5.5238e-02,  9.0312e-01,\n",
      "         -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
      "         -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
      "         -2.6726e-01, -5.4679e-01,  9.3830e-04, -1.0502e-02, -4.6837e-01,\n",
      "          3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
      "          8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
      "         -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
      "         -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6621e-02,  8.5571e-01,\n",
      "         -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
      "          8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
      "          3.3430e-01,  2.1397e-01,  3.0628e-02,  9.3878e-01,  9.9986e-01,\n",
      "          9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
      "          9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
      "          2.3399e-02, -1.0000e+00, -6.1939e-02,  1.9563e-01, -9.0552e-01,\n",
      "         -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
      "          8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4074e-01,  9.6085e-01,\n",
      "          1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
      "         -5.9538e-01,  3.7464e-01, -2.1190e-01,  8.8024e-01,  1.9648e-02,\n",
      "         -3.8349e-01, -8.4779e-01,  1.4677e-02, -2.8376e-02, -4.4313e-01,\n",
      "         -9.4966e-01, -6.5704e-02, -7.2325e-02,  6.5967e-01, -1.1504e-01,\n",
      "          2.1876e-01, -5.5254e-01,  9.2219e-02, -5.0583e-01, -5.2826e-02,\n",
      "          5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
      "         -3.1654e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
      "          1.0000e+00,  1.1756e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
      "         -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
      "          2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
      "          7.8267e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
      "         -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
      "         -4.9026e-01, -1.2517e-01, -3.9077e-02,  8.8128e-02, -8.2481e-01,\n",
      "          3.8301e-01,  1.8045e-01,  5.4796e-02,  8.0041e-01, -3.3501e-01,\n",
      "         -3.9115e-01,  1.4233e-01, -9.0142e-02,  3.4585e-01,  4.4044e-01,\n",
      "          3.1045e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
      "         -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0751e-02,  8.3376e-02,\n",
      "         -3.1307e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
      "         -1.4267e-01, -7.4000e-01,  2.2857e-01, -7.9442e-01, -9.8812e-01,\n",
      "          4.3592e-01,  7.7230e-02, -3.8085e-02,  9.9490e-01,  3.2616e-01,\n",
      "          6.7990e-02,  8.2889e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
      "          3.7667e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
      "         -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
      "          2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
      "          7.3959e-02,  3.1274e-02,  1.0000e+00, -7.5631e-01,  3.5396e-01,\n",
      "          5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
      "          3.5376e-02, -4.6062e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
      "          3.4664e-02,  4.5233e-03, -8.8296e-01, -9.8300e-01,  1.6467e-01,\n",
      "          3.3596e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
      "          1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
      "          2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9312e-01,  8.2643e-01,\n",
      "          3.1895e-02,  1.8035e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
      "         -4.1523e-01, -2.0335e-01,  5.0973e-03,  1.0000e+00, -1.3769e-01,\n",
      "          4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
      "          5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1691e-01,  3.3706e-01,\n",
      "         -9.4996e-02,  3.3125e-01, -1.1600e-01,  5.5664e-02,  6.9017e-01,\n",
      "          5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
      "          4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
      "          4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5116e-02, -9.8683e-01,\n",
      "         -3.4660e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
      "          4.0561e-01,  5.1398e-01, -7.1927e-03,  3.7469e-01, -4.9920e-02,\n",
      "         -1.8379e-01,  1.0699e-01,  6.4272e-02,  9.4363e-01, -4.5982e-01,\n",
      "         -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
      "         -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
      "         -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7385e-02,  9.4459e-01,\n",
      "          8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5496e-02,  2.9531e-01,\n",
      "         -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
      "          9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
      "          1.1140e-01, -7.1072e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
      "          7.1898e-03,  9.3813e-02,  1.2718e-01, -4.2176e-01,  6.2383e-01,\n",
      "         -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
      "          5.2774e-01, -6.4999e-02,  3.8217e-02, -1.3764e-01,  1.3114e-01,\n",
      "         -8.2896e-01, -6.2802e-02, -1.3078e-01, -9.9745e-01,  3.8189e-01,\n",
      "         -1.0000e+00, -4.9527e-02, -3.3011e-01, -9.7047e-03,  7.4032e-01,\n",
      "          4.5588e-01, -4.3037e-02, -5.9485e-01,  3.5136e-02,  8.4290e-01,\n",
      "          7.0024e-01,  4.9499e-03,  1.5221e-01, -4.8182e-01,  3.4913e-02,\n",
      "          6.8681e-02,  5.9797e-02,  9.4146e-02,  5.7532e-01,  3.5063e-02,\n",
      "          1.0000e+00, -4.4780e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
      "         -4.8242e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
      "         -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
      "         -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2238e-01,  2.1811e-01,\n",
      "         -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0505e-02,  9.8319e-01,\n",
      "          3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1440e-02,  9.3666e-01,\n",
      "          7.3989e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
      "         -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
      "          2.1140e-01, -3.4106e-02,  8.0942e-01,  4.9840e-03,  8.9624e-01,\n",
      "          6.7184e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
      "         -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
      "          1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
      "         -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
      "          4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
      "         -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0548e-01,\n",
      "          2.5306e-01, -4.0736e-01, -3.4586e-02,  4.0999e-01, -8.3874e-01,\n",
      "         -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
      "         -5.9613e-01, -1.9441e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
      "          5.9324e-01, -1.8357e-02, -6.7999e-03,  2.1554e-01,  8.2484e-01,\n",
      "          8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
      "          2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3630e-03, -3.2061e-02,\n",
      "         -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2645e-01,  6.3986e-01,\n",
      "         -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
      "          6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3170e-01,  5.3485e-02,\n",
      "          8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
      "         -5.3295e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1457e-01,\n",
      "         -6.7958e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
      "         -6.4847e-02, -4.3871e-01,  6.3752e-01, -2.1017e-01, -2.9291e-01,\n",
      "         -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4510e-02,\n",
      "         -2.9022e-01, -2.1886e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
      "         -9.9998e-01,  1.4066e-01,  9.1604e-02,  1.1458e-01, -2.1965e-01,\n",
      "          3.0746e-01, -5.7720e-02, -8.7692e-01, -9.3892e-02,  2.2809e-01,\n",
      "          3.8768e-02, -3.2828e-01, -3.1139e-01,  4.1117e-01,  4.6004e-01,\n",
      "          5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
      "         -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30d2302c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "         ...,\n",
       "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "039c0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, BertLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "loss = outputs.loss\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb0c65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(**inputs,max_length = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92eda2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,  1012,  1012,\n",
      "          1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
      "          1012,  1012,  1012,  1012]])\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f02eb936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, my dog is cute [SEP]................'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8bbc2936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, BertLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\",return_dict=True, is_decoder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ddea37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"The Gulf War was a conflict between [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f16ebdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1996, 6084, 2162, 2001, 1037, 4736, 2090,  103,  102, 1012, 1012,\n",
      "         1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**tokenized, max_length=24)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99e0ee0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the gulf war was a conflict between [MASK] [SEP]..............'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff1a0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertLMHeadModel.from_pretrained('bert-base-uncased',\n",
    "return_dict=True, is_decoder = True)\n",
    "text = \"A knife is very [MASK].\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "output = model(**input).logits[:, -1, :]\n",
    "softmax = F.softmax(output, -1)\n",
    "index = torch.argmax(softmax, dim = -1)\n",
    "x = tokenizer.decode(index)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94df0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  3007,  1997,  3290,  1010,   103,  1010,  3397,  1996,\n",
      "          9815,  2078, 14255,  6444,  3593,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "(tensor([6]),)\n",
      "tensor([[[ -6.5313,  -6.5047,  -6.5070,  ...,  -5.8149,  -5.6642,  -3.9375],\n",
      "         [-14.2671, -14.6383, -14.6215,  ..., -13.1567, -11.7743, -13.8698],\n",
      "         [ -9.7405, -10.3442, -10.0185,  ...,  -9.2343,  -8.1395, -14.4183],\n",
      "         ...,\n",
      "         [ -5.2318,  -5.9998,  -5.7009,  ...,  -5.4847,  -6.5367,  -5.8576],\n",
      "         [-12.6762, -12.1904, -12.8040,  ..., -10.4086, -10.6798,  -7.9790],\n",
      "         [-14.0402, -14.5983, -14.1129,  ..., -13.5131, -13.5052,  -8.8684]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 17, 30522])\n",
      "tensor([[[5.3086e-07, 5.4518e-07, 5.4390e-07,  ..., 1.0867e-06,\n",
      "          1.2634e-06, 7.1028e-06],\n",
      "         [4.5584e-16, 3.1449e-16, 3.1982e-16,  ..., 1.3838e-15,\n",
      "          5.5139e-15, 6.7821e-16],\n",
      "         [5.5878e-13, 3.0555e-13, 4.2318e-13,  ..., 9.2696e-13,\n",
      "          2.7706e-12, 5.1962e-15],\n",
      "         ...,\n",
      "         [2.2535e-10, 1.0455e-10, 1.4097e-10,  ..., 1.7500e-10,\n",
      "          6.1113e-11, 1.2052e-10],\n",
      "         [7.1462e-17, 1.1616e-16, 6.2893e-17,  ..., 6.9004e-16,\n",
      "          5.2618e-16, 7.8356e-15],\n",
      "         [3.1780e-11, 1.8187e-11, 2.9551e-11,  ..., 5.3837e-11,\n",
      "          5.4261e-11, 5.6006e-09]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 17, 30522])\n",
      "top 10 \n",
      "tensor([23741, 28631, 22887, 27452, 26923, 28480, 26843,  3290,  9815, 16938])\n",
      "decoded \n",
      "['veracruz', 'yucatan', 'guadalajara', 'puebla', 'oaxaca', 'chihuahua', 'monterrey', 'mexico', 'maya', 'guerrero']\n",
      "The capital of Mexico, veracruz, contains the Mayan piramid.\n",
      "The capital of Mexico, yucatan, contains the Mayan piramid.\n",
      "The capital of Mexico, guadalajara, contains the Mayan piramid.\n",
      "The capital of Mexico, puebla, contains the Mayan piramid.\n",
      "The capital of Mexico, oaxaca, contains the Mayan piramid.\n",
      "The capital of Mexico, chihuahua, contains the Mayan piramid.\n",
      "The capital of Mexico, monterrey, contains the Mayan piramid.\n",
      "The capital of Mexico, mexico, contains the Mayan piramid.\n",
      "The capital of Mexico, maya, contains the Mayan piramid.\n",
      "The capital of Mexico, guerrero, contains the Mayan piramid.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path= '../bert-base-uncased')\n",
    "# bert model for masked language modelling\n",
    "model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path = '../bert-base-uncased',    return_dict = True)\n",
    "# return_dict True to use mask token\n",
    "text = \"The capital of Mexico, \" + tokenizer.mask_token + \", contains the Mayan piramid.\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "print(input)\n",
    "# get index of the mask\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "print(mask_index)\n",
    "output = model(**input)\n",
    "# logits are output of BERT model before softmax activation\n",
    "logits = output.logits\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "print(softmax)\n",
    "print(softmax.shape)\n",
    "\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "print('top 10 ')\n",
    "\n",
    "print(top_10)\n",
    "print('decoded ')\n",
    "\n",
    "print([tokenizer.decode([token]) for token in top_10])\n",
    "for token in top_10:\n",
    "   word = tokenizer.decode([token])\n",
    "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "   print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d9f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = top_10.add(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f805a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23741, 28631, 22887, 27452, 26923, 28480, 26843,  3290,  9815, 16938])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e85985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [13420, 22108,  8224,  9980,  7513, 11742,  9733,  6522, 10321, 23853]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be9b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intel', 'siemens', 'google', 'ibm', 'microsoft', 'toyota', 'amazon', 'hp', 'boeing', 'lego']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "The decoded sentence.\n",
    "\n",
    "Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove\n",
    "special tokens and clean up tokenization spaces.\n",
    "\"\"\"\n",
    "print([tokenizer.decode([token]) for token in a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1508e8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intel',\n",
       " 'siemens',\n",
       " 'google',\n",
       " 'ibm',\n",
       " 'microsoft',\n",
       " 'toyota',\n",
       " 'amazon',\n",
       " 'hp',\n",
       " 'boeing',\n",
       " 'lego']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c96beaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intel siemens google ibm microsoft toyota amazon hp boeing lego'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d85df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
